---
title: "EdX Data Science Capstone Project"
author: "William Mee"
date: '2022-06-27'
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
load(file="/Users/WillMee/dev/personal/edx-datasci/capstone/data/movielens-capstone.Rda")
load(file="/Users/WillMee/dev/personal/edx-datasci/capstone/data/movielens-capstone-validation.Rda")
```

# Summary

This is the machine learning capstone project for EdX Data Science online course.

I elected to implement a recommendation system based on the 
[Movielens 10 million dataset](https://grouplens.org/datasets/movielens/10m/).
I began by following the bias models we explored in coursework, took this further
by also using genre as a feature, and then 
followed the hint given in coursework to explore the some of the more advanced models of the 
[R recommenderlab package](https://www.rdocumentation.org/packages/recommenderlab/versions/1.0.1)
in particular, looking at use of SVD.

## Baseline: mean of all ratings
I began by establishing a simple RMSE function and calculating a baseline RMSE 
by taking a  simple mean of all the ratings: this value is **1.061202**. 

| Model # | Description     | RMSE      | 
|:--------|:----------------|:---------:|
| 0       | Baseline        | 1.061202  |

```{r}
rmse <- function(true_ratings, predicted_ratings) {
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

mu_hat <- mean(edx$rating)
pred_0 <- rep(mu_hat, nrow(validation))
rmse(validation$rating, pred_0)
```

## Bias models (users, movies)
I proceeded to calculate the bias successively of movies and users. Code
snippets are below. Using this approach, I obtained the following RMSE values,
which got progressively better. 

| Model # | Description     | RMSE      | 
|:--------|:----------------|:---------:|
| 1       | Movie Bias      | 0.9439087 |
| 2       | Movie/User Bias | 0.8653488 |

### Movie Bias
```{r pred_1}
movie_bias <- edx %>%
  group_by(movieId) %>%
  summarize(biasMovie = mean(rating)-mu_hat)

pred_1 <- validation %>%
  left_join(movie_bias, by='movieId') %>%
  pull(biasMovie)

pred_1 <- pred_1 + mu_hat

# Prediction 1: 0.9439087
rmse(validation$rating, pred_1)
```

### Movie/User Bias
```{r}
user_bias <- edx %>%
  left_join(movie_bias, by='movieId') %>%
  group_by(userId) %>%
  summarize(biasUser = mean(rating - mu_hat - biasMovie)) 

pred_2 <- validation %>%
  left_join(movie_bias, by='movieId') %>%
  left_join(user_bias, by='userId') %>%
  mutate(prediction = mu_hat + biasMovie + biasUser) %>%
  pull(prediction)

# Prediction 2: 0.8653488
rmse(validation$rating, pred_2)
```

## Using regularization on the Bias Models
In both cases, using regularization on the bias models improved results. The 
table below repeats the above models for comparison.

| Model # | Description         | RMSE      | RMSE w regularization |
|:--------|:----------------    |:---------:|:---------------------:|
| 3       | Movie Bias reg      | 0.9439087 |  0.9438542 |
| 4       | Movie/User Bias reg | 0.8653488 | 0.8648177 |

## Using Genre as a feature

The Movielens data associates zero or more genres with a movie. I used this
as an additional feature.

| Movie/User/Genre Bias | ? | 0.8647045 | 

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
